{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "969dbfed-06ab-40e6-9d25-256045ef8870",
   "metadata": {},
   "source": [
    "## 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bf56fa-aab0-4039-a5d0-1be3de022603",
   "metadata": {},
   "source": [
    "Simple Linear Regression:\n",
    "Simple Linear Regression is a statistical method used to model the relationship between a single independent variable (predictor) and a dependent variable (response). The relationship is modeled as a linear equation, typically expressed as:\n",
    "\n",
    "Y=β 0+β 1 + X+ϵ\n",
    "\n",
    "Example : \n",
    "-Number of hours studied by student vs marks \n",
    "\n",
    "\n",
    "Multiple Linear Regression:\n",
    "\n",
    "Multiple Linear Regression extends the concept of simple linear regression to include more than one independent variable. It models the relationship between a dependent variable and multiple independent variables:\n",
    "\n",
    "Y=β 0+β 1*X 1+β 2*X2+…+β n* X n+ϵ\n",
    "\n",
    "Example :\n",
    "-House size , Number of rooms ,House Price "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5b2adb-46c5-4e71-bc4d-8d1039494fcb",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35731c3-e57f-485e-adb0-0d8be90541b5",
   "metadata": {},
   "source": [
    "Linear regression makes several assumptions about the underlying data. It's important to check these assumptions to ensure the validity of the model and the reliability of the results. Here are the key assumptions of linear regression:\n",
    "\n",
    "Linearity:\n",
    "\n",
    "Assumption: The relationship between the independent variable(s) and the dependent variable is linear.\n",
    "Check: Scatter plots of the variables can help visualize their relationship. If the points roughly form a straight line, the linearity assumption may hold.\n",
    "\n",
    "Independence of Errors:\n",
    "\n",
    "Assumption: Residuals (the differences between observed and predicted values) are independent of each other.\n",
    "Check: Plot residuals against the predicted values. There should be no discernible pattern, and residuals should not be correlated.\n",
    "\n",
    "Homoscedasticity (Constant Variance of Errors):\n",
    "\n",
    "Assumption: The variance of the residuals is constant across all levels of the independent variable(s).\n",
    "Check: Scatter plots of residuals against predicted values or the independent variable(s). A \"fan\" or \"funnel\" shape suggests heteroscedasticity, violating the assumption.\n",
    "\n",
    "Normality of Residuals:\n",
    "\n",
    "Assumption: Residuals are normally distributed.\n",
    "Check: Histograms or Q-Q plots of residuals. If the residuals are close to normally distributed, the assumption may be satisfied. Alternatively, statistical tests like the Shapiro-Wilk test can be used.\n",
    "\n",
    "No Perfect Multicollinearity:\n",
    "\n",
    "Assumption: Independent variables are not perfectly correlated with each other.\n",
    "Check: Calculate variance inflation factor (VIF) for each variable. High VIF values indicate potential multicollinearity.\n",
    "\n",
    "\n",
    "No Autocorrelation:\n",
    "\n",
    "Assumption: Residuals are not correlated with each other (no patterns in the residuals over time or across observations).\n",
    "Check: Plot residuals against time or observation order. Alternatively, use statistical tests like the Durbin-Watson test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38ac983-d9e4-468d-9d72-2e7188af0f4f",
   "metadata": {},
   "source": [
    "## 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbb4a6b-8b20-4ffb-9249-af11637304c3",
   "metadata": {},
   "source": [
    "In a linear regression model, the slope and intercept have specific interpretations:\n",
    "\n",
    "Intercept (\n",
    "\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    " ):\n",
    "\n",
    "The intercept represents the predicted value of the dependent variable when all independent variables are set to zero.\n",
    "It is the starting point of the regression line on the y-axis.\n",
    "Slope (\n",
    "\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    " ):\n",
    "\n",
    "The slope represents the change in the dependent variable for a one-unit change in the independent variable.\n",
    "It defines the steepness of the regression line.\n",
    "Example: Predicting Salary Based on Years of Experience\n",
    "\n",
    "Let's consider a real-world scenario where you want to predict an employee's salary based on their years of experience. The linear regression model can be represented as:\n",
    "\n",
    "Salary =\n",
    "\n",
    "0\n",
    "+\n",
    "\n",
    "1\n",
    "×\n",
    "Years_of_Experience\n",
    "+\n",
    "\n",
    "Salary=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " ×Years_of_Experience+ϵ\n",
    "\n",
    "Interpretation of Intercept (\n",
    "\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    " ):\n",
    "\n",
    "If Years_of_Experience is zero, the predicted salary is the value of the intercept.\n",
    "In practical terms, this is the expected salary for someone with zero years of experience. However, the interpretation may not be meaningful if zero years of experience is unrealistic in the context (e.g., a new graduate).\n",
    "Interpretation of Slope (\n",
    "\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    " ):\n",
    "\n",
    "For every additional year of experience, the predicted salary increases by the value of the slope.\n",
    "If \n",
    "\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    "  is, for example, $5,000, it means that, on average, each additional year of experience is associated with a $5,000 increase in salary.\n",
    "Example Calculation:\n",
    "\n",
    "Let's say the regression analysis yields the following results:\n",
    "\n",
    "Intercept (\n",
    "\n",
    "0\n",
    "β \n",
    "0\n",
    "​\n",
    " ) = $50,000\n",
    "Slope (\n",
    "\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    " ) = $5,000\n",
    "If an employee has 3 years of experience (\n",
    "Years_of_Experience=\n",
    "3\n",
    "Years_of_Experience=3), the predicted salary would be:\n",
    "\n",
    "Predicted Salary=\n",
    "$\n",
    "50\n",
    ",\n",
    "000\n",
    "+\n",
    "(\n",
    "$\n",
    "5\n",
    ",\n",
    "000\n",
    "×\n",
    "3\n",
    ")\n",
    "=\n",
    "$\n",
    "65\n",
    ",\n",
    "000\n",
    "Predicted Salary=$50,000+($5,000×3)=$65,000\n",
    "\n",
    "So, based on this model, an employee with 3 years of experience is predicted to have a salary of $65,000."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64aec5e-23a4-492b-94c6-953b44dfde38",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad54866f-bf2e-4c91-9027-462548aa4c8b",
   "metadata": {},
   "source": [
    "Gradient Descent is an iterative optimization algorithm used to minimize the cost function in machine learning models. The goal of machine learning is often to find the optimal parameters (weights and biases) for a model that minimizes the difference between the predicted and actual values. Gradient Descent helps in finding these optimal parameters by iteratively moving towards the minimum of the cost function.\n",
    "\n",
    "Basic Idea:\n",
    "\n",
    "Cost Function:\n",
    "\n",
    "In machine learning, the performance of a model is quantified by a cost function or loss function. It measures the difference between the predicted values and the actual values.\n",
    "Optimization:\n",
    "\n",
    "The objective is to find the values of parameters (weights and biases) that minimize the cost function.\n",
    "Gradient Descent:\n",
    "\n",
    "Gradient Descent starts with an initial guess for the parameters.\n",
    "It calculates the gradient (partial derivatives) of the cost function with respect to each parameter.\n",
    "The algorithm then updates the parameters in the opposite direction of the gradient to reduce the cost.\n",
    "This process is repeated iteratively until convergence or a predefined number of iterations.\n",
    "Steps of Gradient Descent:\n",
    "\n",
    "Initialization:\n",
    "\n",
    "Initialize the parameters (weights and biases) randomly or with some predefined values.\n",
    "Compute the Gradient:\n",
    "\n",
    "Calculate the gradient (partial derivatives) of the cost function with respect to each parameter.\n",
    "Update Parameters:\n",
    "\n",
    "Update the parameters in the opposite direction of the gradient using a learning rate.\n",
    "Learning rate (\n",
    "�\n",
    "α) controls the size of the step taken in the parameter space.\n",
    "Repeat:\n",
    "\n",
    "Repeat steps 2 and 3 until convergence or a maximum number of iterations is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54314c7-3237-48ba-a228-5e0e8dd4cb15",
   "metadata": {},
   "source": [
    "## 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18590dff-8d55-4992-9859-13c10230ccbc",
   "metadata": {},
   "source": [
    "Multiple Linear Regression is an extension of simple linear regression that allows for modeling the relationship between a dependent variable (\n",
    "\n",
    "Y) and multiple independent variables (\n",
    "X\n",
    "1\n",
    ",\n",
    "X\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "X \n",
    "1\n",
    "​\n",
    " ,X \n",
    "2\n",
    "​\n",
    " ,...,X \n",
    "n\n",
    "​\n",
    " ). The model can be represented by the following equation:\n",
    " \n",
    " Y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X \n",
    "1\n",
    "​\n",
    " +β \n",
    "2\n",
    "​\n",
    " X \n",
    "2\n",
    "​\n",
    " +...+β \n",
    "n\n",
    "​\n",
    " X \n",
    "n\n",
    "​\n",
    " +ϵ\n",
    " \n",
    " \n",
    "\n",
    "Differences from Simple Linear Regression:\n",
    "\n",
    "Number of Independent Variables:\n",
    "\n",
    "In simple linear regression, there is only one independent variable (\n",
    "\n",
    "X).\n",
    "In multiple linear regression, there are two or more independent variables (\n",
    "x\n",
    "1\n",
    ",\n",
    "x\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "\n",
    "X \n",
    "1\n",
    "​\n",
    " ,X \n",
    "2\n",
    "​\n",
    " ,...,X \n",
    "n\n",
    "​\n",
    " ).\n",
    "Equation:\n",
    "\n",
    "Simple Linear Regression: \n",
    "�\n",
    "=\n",
    "x\n",
    "0\n",
    "+\n",
    "x\n",
    "1\n",
    "\n",
    "+\n",
    "\n",
    "Y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X+ϵ\n",
    "Multiple Linear Regression: \n",
    "\n",
    "=\n",
    "x\n",
    "0\n",
    "+\n",
    "x\n",
    "1\n",
    "β\n",
    "1\n",
    "+\n",
    "β\n",
    "2\n",
    "x\n",
    "2\n",
    "+\n",
    ".\n",
    ".\n",
    ".\n",
    "+\n",
    "\n",
    "\n",
    "+\n",
    "n \n",
    "Y=β \n",
    "0\n",
    "​\n",
    " +β \n",
    "1\n",
    "​\n",
    " X \n",
    "1\n",
    "​\n",
    " +β \n",
    "2\n",
    "​\n",
    " X \n",
    "2\n",
    "​\n",
    " +...+β \n",
    "n\n",
    "​\n",
    " X \n",
    "n\n",
    "​\n",
    " +ϵ\n",
    "Interpretation of Coefficients:\n",
    "\n",
    "In simple linear regression, the coefficient (\n",
    "x\n",
    "1\n",
    "β \n",
    "1\n",
    "​\n",
    " ) represents the change in the dependent variable (\n",
    "\n",
    "Y) for a one-unit change in the independent variable (\n",
    "\n",
    "X).\n",
    "In multiple linear regression, each coefficient (\n",
    "x\n",
    "1\n",
    ",\n",
    "x\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "β \n",
    "1\n",
    "​\n",
    " ,β \n",
    "2\n",
    "​\n",
    " ,...,β \n",
    "n\n",
    "​\n",
    " ) represents the change in the dependent variable (\n",
    "\n",
    "Y) for a one-unit change in the corresponding independent variable (\n",
    "x\n",
    "1\n",
    ",\n",
    "x\n",
    "2\n",
    ",\n",
    ".\n",
    ".\n",
    ".\n",
    ",\n",
    "\n",
    "\n",
    "X \n",
    "1\n",
    "​\n",
    " ,X \n",
    "2\n",
    "​\n",
    " ,...,X \n",
    "n\n",
    "​\n",
    " ), while holding other variables constant.\n",
    "Complexity:\n",
    "\n",
    "Multiple linear regression is more complex than simple linear regression due to the presence of multiple independent variables.\n",
    "The model becomes a hyperplane in a multidimensional space, and the interpretation becomes more intricate.\n",
    "Assumptions:\n",
    "\n",
    "Multiple linear regression assumes the same assumptions as simple linear regression (linearity, independence, homoscedasticity, normality of residuals), but the complexity of checking and addressing these assumptions increases with the number of independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dac9a0e-e305-43df-89c7-9fbeb0116207",
   "metadata": {},
   "source": [
    "## 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e242829b-0878-43a0-8169-5697c582dde5",
   "metadata": {},
   "source": [
    "Multicollinearity refers to a situation in multiple linear regression when two or more independent variables in the model are highly correlated with each other. It can cause issues in the estimation of the regression coefficients and their interpretation. Multicollinearity does not affect the predictive power of the model, but it can make the individual contributions of correlated variables difficult to distinguish.\n",
    "\n",
    "\n",
    "Multicollinearity in Multiple Linear Regression:\n",
    "\n",
    "Multicollinearity refers to a situation in multiple linear regression when two or more independent variables in the model are highly correlated with each other. It can cause issues in the estimation of the regression coefficients and their interpretation. Multicollinearity does not affect the predictive power of the model, but it can make the individual contributions of correlated variables difficult to distinguish.\n",
    "\n",
    "Signs of Multicollinearity:\n",
    "\n",
    "High Correlation Coefficients:\n",
    "\n",
    "Look for high absolute values of correlation coefficients between pairs of independent variables.\n",
    "Large Variance Inflation Factor (VIF):\n",
    "\n",
    "VIF measures how much the variance of the estimated regression coefficients increases when your predictors are correlated.\n",
    "A high VIF (usually greater than 10) indicates potential multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9be97d4-22d6-4144-9638-5ac3f5327755",
   "metadata": {},
   "source": [
    "## 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb76d8d-19b7-43d1-88b0-b47c9bda8eb5",
   "metadata": {},
   "source": [
    "The polynomial regression equation of degree \\(n\\) is given by:\n",
    "Polynomial Regression is a type of regression analysis where the relationship between the independent variable (\n",
    "X) and the dependent variable (\n",
    "Y) is modeled as an \n",
    "n-th degree polynomial. The general form of a polynomial regression equation of degree \n",
    "�\n",
    "n is:\n",
    "\n",
    "\\[ Y = \\beta_0 + \\beta_1X + \\beta_2X^2 + \\ldots + \\beta_nX^n + \\epsilon \\]\n",
    "\n",
    "- \\(Y\\) is the dependent variable.\n",
    "- \\(X\\) is the independent variable.\n",
    "- \\(\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_n\\) are the coefficients.\n",
    "- \\(X^2, X^3, \\ldots, X^n\\) represent the higher-degree terms.\n",
    "- \\(\\epsilon\\) is the error term.\n",
    "\n",
    "Linearity vs. Non-Linearity:\n",
    "\n",
    "Linear Regression assumes a linear relationship between the independent and dependent variables.\n",
    "Polynomial Regression allows for a more flexible, non-linear relationship by introducing higher-degree terms.\n",
    "Model Complexity:\n",
    "\n",
    "Linear Regression models linear relationships and is appropriate when the data shows a linear trend.\n",
    "Polynomial Regression can capture more complex relationships and is suitable when the relationship between variables is curved or nonlinear.\n",
    "Underfitting and Overfitting:\n",
    "\n",
    "Linear Regression may underfit complex patterns in the data.\n",
    "Polynomial Regression has the potential to overfit the data if the degree of the polynomial is too high, capturing noise in the data.\n",
    "Degree of the Polynomial:\n",
    "\n",
    "In Polynomial Regression, the degree of the polynomial (\n",
    "�\n",
    "n) is a parameter that needs to be chosen carefully.\n",
    "A higher degree allows the model to fit the training data more closely, but it may lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef63d01d-15b7-4d07-99e4-848b675eb4bc",
   "metadata": {},
   "source": [
    "## 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e122f5-14d0-4b87-b114-51809cf1de40",
   "metadata": {},
   "source": [
    "\n",
    "Advantages of Polynomial Regression:\n",
    "\n",
    "Captures Nonlinear Relationships:\n",
    "\n",
    "Polynomial regression can capture more complex relationships between variables compared to linear regression. It is capable of fitting curves and capturing nonlinear patterns in the data.\n",
    "Flexibility:\n",
    "\n",
    "The inclusion of higher-degree terms provides flexibility in modeling various shapes of curves. This flexibility allows the model to adapt to the underlying complexity of the data.\n",
    "Improved Fit:\n",
    "\n",
    "Polynomial regression can provide a better fit to the training data, especially when the true relationship is not strictly linear.\n",
    "Useful for Small Datasets:\n",
    "\n",
    "In situations where the dataset is small, and linear models may underfit, polynomial regression can offer a more expressive model.\n",
    "Disadvantages of Polynomial Regression:\n",
    "\n",
    "Overfitting:\n",
    "\n",
    "One of the main challenges with polynomial regression is the risk of overfitting, especially when the degree of the polynomial is high. The model may capture noise and fluctuations in the training data, leading to poor generalization on new data.\n",
    "Increased Complexity:\n",
    "\n",
    "As the degree of the polynomial increases, the model becomes more complex. This complexity can make the interpretation of the model and the coefficients challenging.\n",
    "Computational Intensity:\n",
    "\n",
    "Polynomial regression, particularly with higher-degree polynomials, can be computationally intensive. The model may require more time and resources for training and prediction.\n",
    "Limited Extrapolation:\n",
    "\n",
    "Extrapolating predictions beyond the range of the training data can be unreliable, as polynomial functions may produce unexpected results outside the observed data range.\n",
    "When to Prefer Polynomial Regression:\n",
    "\n",
    "Nonlinear Relationships:\n",
    "\n",
    "Use polynomial regression when there is evidence or belief that the relationship between variables is nonlinear.\n",
    "Capturing Curvature:\n",
    "\n",
    "When the data shows a curved pattern rather than a straight line, polynomial regression can better capture the curvature.\n",
    "Small to Moderate Degrees:\n",
    "\n",
    "Prefer polynomial regression with a moderate degree to capture complexities without introducing excessive overfitting. The choice of the degree should be guided by cross-validation.\n",
    "Supplementary to Linear Regression:\n",
    "\n",
    "Polynomial regression can be used in conjunction with linear regression to explore both linear and nonlinear relationships. For example, including quadratic terms alongside linear terms in a model.\n",
    "Exploratory Data Analysis:\n",
    "\n",
    "In the early stages of analysis, when the true nature of the relationship is unclear, polynomial regression can be used for exploratory purposes to reveal potential patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2308a95-a02e-4fff-aa39-dbd00f6e406a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
